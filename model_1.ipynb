{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABOUT THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "NAME          : model_1 \n",
    "DATASET       : FMNIST\n",
    "MODEL         : Two layered CNN with Max-pooling and Valid-padding\n",
    "ACCURACY      : 90.37 %\n",
    "DATASET CLASS : standard\n",
    "'''\n",
    "# Model Name\n",
    "model_name = 'model_1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NOTE__ : The model has been analysed after 90,000 iterations. The results of these iterations, as well as the parameter values can be restored by running the second last cell of this notebook.\n",
    "\n",
    "To restore the results, run all the cells before the section \"Training the model\", and then run the second last cell. Thereafter you can run all the cells after \"Evaluating model performance\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRELIMINARY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''IMPORTING MODULES'''\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import os\n",
    "from skimage import io, transform\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOADING THE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''LOADING DATASET'''\n",
    "# Dataset located in folder : \"./data\"\n",
    "\n",
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''MAKING DATASET ITERABLE'''\n",
    "\n",
    "batch_size = 100\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATING THE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a model class to design the model. The model has the following layers:-\n",
    "- __CONVOLUTION 1__\n",
    "    - In channels : 01\n",
    "    - Out channels : 16\n",
    "- __MAX POOL 1__\n",
    "    - Kernel size : 2\n",
    "- __CONVOLUTION 2__\n",
    "    - In channels : 16\n",
    "    - Out channels : 32\n",
    "- __MAX POOL 2__\n",
    "    - Kernel size : 2\n",
    "\n",
    "- __FULLY CONNECTED__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''CREATEING MODEL CLASS'''\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        \n",
    "        # Convolution 1 [01,28,28 -> 16,24,24]\n",
    "        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=0)\n",
    "        self.relu1 = nn.ReLU() \n",
    "        \n",
    "        # Max pool 1 [16,24,24 -> 16,12,12]\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
    "     \n",
    "        # Convolution 2 [16,12,12 -> 32,08,08]\n",
    "        self.cnn2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=0)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        # Max pool 2 [32,08,08 -> 32,04,04]\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # Fully connected (readout) [32*4*4 -> 10]\n",
    "        self.fc1 = nn.Linear(32 * 4 * 4, 10) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Convolution 1\n",
    "        out = self.cnn1(x)\n",
    "        out = self.relu1(out)\n",
    "        \n",
    "        # Max pool 1\n",
    "        out = self.maxpool1(out)\n",
    "        \n",
    "        # Convolution 2 \n",
    "        out = self.cnn2(out)\n",
    "        out = self.relu2(out)\n",
    "        \n",
    "        # Max pool 2 \n",
    "        out = self.maxpool2(out)\n",
    "        \n",
    "        # Resize [100,32,04,04 -> 100,32*4*4]\n",
    "        out = out.view(out.size(0), -1)    \n",
    "        \n",
    "        # Linear function (readout)\n",
    "        out = self.fc1(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''INSTANTIATING MODEL CLASS'''\n",
    "\n",
    "model = CNNModel()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''INSTANTIATING LOSS CLASS'''\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''INSTANTIATING OPTIMIZER CLASS'''\n",
    "\n",
    "learning_rate = 0.03\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''INITIALIZING SOME VARIABLES'''\n",
    "\n",
    "iterr = 0\n",
    "\n",
    "# Initializing inspection lists\n",
    "iter_list = []                           # Saves Iterations at which the model has been evaluated\n",
    "train_loss_list = []                     # Saves Train Loss\n",
    "train_acc_list = []                      # Saves Train Accuracy\n",
    "test_loss_list = []                      # Saves Test Loss\n",
    "test_acc_list = []                       # Saves Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''SETTING CUSTOM PARAMETERS'''\n",
    "\n",
    "n_iters = 18000                          # Number of iteration to train the model\n",
    "inspect_size = 250                       # Size at which model is evaluated for later inspection\n",
    "num_epochs = int(n_iters / (len(train_dataset) / batch_size))\n",
    "\n",
    "print(\"Number of Iterations     :\", n_iters)\n",
    "print(\"Number of Epochs         :\", num_epochs)\n",
    "print(\"Number of Sample-Points  :\", int(n_iters/inspect_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''TRAINING THE MODEL'''\n",
    "\n",
    "import time\n",
    "time_begin = time.asctime()   # Time when training started\n",
    "\n",
    "init_iters = iterr\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            images = Variable(images.cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "        else:\n",
    "            images = Variable(images)\n",
    "            labels = Variable(labels)\n",
    "        \n",
    "        optimizer.zero_grad()                 # Clearing the previous gradients\n",
    "\n",
    "        outputs = model(images)               # Forward propogation\n",
    "        loss = criterion(outputs, labels)     # Calculating the Train loss\n",
    "        loss.backward()                       # Backward propogation\n",
    "        optimizer.step()                      # Optimizing the parameters\n",
    "        \n",
    "        iterr += 1\n",
    "        \n",
    "        ### Inspecting the performance of the model ###\n",
    "        if iterr % inspect_size == 0:\n",
    "            iter_list.append(iterr)\n",
    "            print(\"Iteration : {:.0f}/{:.0f} [{:2.0f}%] \".format(iterr - init_iters, n_iters, 100*(iterr - init_iters)/n_iters))\n",
    "            print('---------------------------')\n",
    "        # --------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "        ### Calculating train accuracy and loss ###\n",
    "            temp, predicted = torch.max(outputs.data, 1)\n",
    "            total = labels.size(0) \n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                correct = (predicted.cpu() == labels.cpu()).sum()\n",
    "            else:\n",
    "                correct = (predicted == labels).sum()\n",
    "                                          \n",
    "            accuracy = 100 * correct.item() / total            \n",
    "\n",
    "            train_loss_list.append(loss.item())\n",
    "            train_acc_list.append(accuracy)\n",
    "\n",
    "            print('[Train]\\t Loss: {:.2f}\\t Accuracy: {:.2f}'.format(loss.item(), accuracy))    \n",
    "        # --------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "        ### Calculating test accuracy and loss ###\n",
    "            correct = 0\n",
    "            total = 0          \n",
    "                        \n",
    "            for images, labels in test_loader:\n",
    "\n",
    "                if torch.cuda.is_available():\n",
    "                    images = Variable(images.cuda())\n",
    "                    labels = Variable(labels.cuda())\n",
    "                else:\n",
    "                    images = Variable(images)\n",
    "                    labels = Variable(labels)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                temp, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    correct += (predicted.cpu() == labels.cpu()).sum()\n",
    "                else:\n",
    "                    correct += (predicted == labels).sum()\n",
    "            \n",
    "            accuracy = 100 * correct.item() / total\n",
    "            \n",
    "            test_loss_list.append(loss.item())\n",
    "            test_acc_list.append(accuracy)\n",
    "            \n",
    "            print('[Test ]\\t Loss: {:.2f}\\t Accuracy: {:.2f}'.format(loss.item(), accuracy))\n",
    "        # --------------------------------------------------------------------------------------------------------\n",
    "            print('=========================================================')\n",
    "time_end = time.asctime()    # Time when training ended "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''POST TRAINING RESULTS'''\n",
    "\n",
    "# Formatting the date-time data\n",
    "from datetime import datetime\n",
    "FMT = '%H:%M:%S'\n",
    "td = (datetime.strptime(time_end[11:19], FMT) - datetime.strptime(time_begin[11:19], FMT)).seconds\n",
    "hr = (td//3600)\n",
    "min = (td - 3600*hr)//60\n",
    "sec = (td - 3600*hr - 60*min)\n",
    "\n",
    "print(\"Total Iterations     : {:.0f}\".format(iterr))\n",
    "print(\"Total Epochs         : {:.0f}\".format(iterr*100/60000))\n",
    "print(\"Total Sample-Points  : {:.0f}\".format(iterr/inspect_size))\n",
    "print(\"-------------------------------\")\n",
    "print(\"Accuracy - Train     : {:.2f}\".format(np.mean(train_acc_list[-10:])))\n",
    "print(\"Accuracy - Test      : {:.2f}\".format(np.mean(test_acc_list[-10:])))\n",
    "print(\"-------------------------------\")\n",
    "print(\"Start Time        : {}\".format(time_begin[11:19]))\n",
    "print(\"End Time          : {}\".format(time_end[11:19]))\n",
    "print(\"Total Train-time  : {:2.0f}:{:2.0f}:{:2.0f}\".format(hr,min,sec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATING MODEL PERFORMANCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the performance of out model, we inspect the data in the inspection lists we had defined earlier. However the performance of the model depends greatly on the the specific data subset chosen for evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''PLOTTING ABSOLUTE VALUES'''\n",
    "\n",
    "plt.figure(figsize=[16,6], dpi=100)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(iter_list, train_acc_list, '-', c='salmon', label='Train Accuracy')\n",
    "plt.plot(iter_list, test_acc_list, '-', c='brown', label='Test Accuracy')\n",
    "plt.legend(fontsize='10')\n",
    "plt.title(\"ABSOLUTE ACCURACY\", size='20')\n",
    "plt.xlabel('Number of Iterations', size='15')\n",
    "plt.ylabel('Accuracy', size='15')\n",
    "plt.grid(True, linestyle='-.')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(iter_list, train_loss_list, '-', c='salmon', label='Train Loss')\n",
    "plt.plot(iter_list, test_loss_list, '-', c='brown', label='Test Loss')\n",
    "plt.legend(fontsize='10')\n",
    "plt.title(\"ABSOLUTE LOSS\", size='20')\n",
    "plt.xlabel('Number of Iterations', size='15')\n",
    "plt.ylabel('Loss', size='15')\n",
    "plt.grid(True, linestyle='-.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For train evaluation we had chosen a random subset of 100 data-points. This brings high variability in the performance of the model. Hence we generate rolling average lists that calculate the accuracy and loss as the average of the last few (= roll_size) data points and plot these as a function of the number of iterations elapsed. This gives us a much smoother and followable trend line.\n",
    "\n",
    "Although we have calculated test accuracy and loss over a larger data subset (10,000 data-points), we still see some variability in the data. Hence we generate rolling data lists for test data. We use fewer data-points (=10) for the test rolling list as it has less variability, and hence gives a smooth curve for fewer data-points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''GENERATING AVERAGE INSPECTION LISTS'''\n",
    "\n",
    "# Defining roll function\n",
    "def make_roll(input_list, roll_size=5):\n",
    "    output_list = []\n",
    "    \n",
    "    for i in range(len(input_list)):\n",
    "        if i==0:\n",
    "            output_list.append(input_list[0])\n",
    "        elif i<roll_size:\n",
    "            output_list.append(np.mean(input_list[:i+1]))\n",
    "        else:\n",
    "            output_list.append(np.mean(input_list[i-roll_size:i]))\n",
    "    return output_list\n",
    "\n",
    "# Generating roll lists\n",
    "train_roll_loss_list = make_roll(train_loss_list, roll_size=30) \n",
    "train_roll_acc_list = make_roll(train_acc_list, roll_size=30)  \n",
    "test_roll_loss_list = make_roll(test_loss_list, roll_size=10)   \n",
    "test_roll_acc_list = make_roll(test_acc_list, roll_size=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''PLOTTING AVERAGE VALUES'''\n",
    "\n",
    "plt.figure(figsize=[16,6], dpi=100)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(iter_list, train_acc_list, '-', c='salmon', label='Train Accuracy')\n",
    "plt.plot(iter_list, train_roll_acc_list, '-r', lw=3, label='Train Accuracy [Avg]')\n",
    "plt.plot(iter_list, test_acc_list, '-', c='brown', label='Test Accuracy')\n",
    "plt.plot(iter_list, test_roll_acc_list, '-k', lw=3, label='Test Accuracy [Avg]')\n",
    "plt.legend(fontsize='10')\n",
    "plt.title(\"AVERAGE ACCURACY\", size='20')\n",
    "plt.xlabel('Number of Iterations', size='15')\n",
    "plt.ylabel('Accuracy', size='15')\n",
    "plt.grid(True, linestyle='-.')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(iter_list, train_loss_list, '-', c='salmon', label='Train Loss')\n",
    "plt.plot(iter_list, train_roll_loss_list, '-r', lw=3, label='Train Loss [Avg]')\n",
    "plt.plot(iter_list, test_loss_list, '-', c='brown', label='Test Loss')\n",
    "plt.plot(iter_list, test_roll_loss_list, '-k', lw=3, label='Test Loss [Avg]')\n",
    "plt.legend(fontsize='10')\n",
    "plt.title(\"AVERAGE LOSS\", size='20')\n",
    "plt.xlabel('Number of Iterations', size='15')\n",
    "plt.ylabel('Loss', size='15')\n",
    "plt.grid(True, linestyle='-.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average accuracy and loss graphs clearly show that our model is successful in fitting the data well. Theses graphs potray a solid trend right through the osccillating values of the previous graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACCURACY ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''PLOTTTING THE ACCURACY GRAPH'''\n",
    "\n",
    "plt.figure(figsize=[39,24], dpi=50)\n",
    "\n",
    "plt.plot(iter_list, train_acc_list, '-', lw=3, c='salmon', label='Train Accuracy')\n",
    "plt.plot(iter_list, train_roll_acc_list, '-|r', lw=7, label='Train Accuracy [Roll]')\n",
    "\n",
    "plt.plot(iter_list, test_acc_list, '-', lw=3, c='brown', label='Test Accuracy')\n",
    "plt.plot(iter_list, test_roll_acc_list, '-|k', lw=7, label='Test Accuracy [Roll]')\n",
    "\n",
    "plt.title('ACCURACY vs ITERATIONS', size='60')\n",
    "plt.xlabel('Number of Iterations', size='40')\n",
    "plt.ylabel('Accuracy', size='40')\n",
    "\n",
    "plt.grid(True, linestyle='-.',)\n",
    "plt.tick_params(labelcolor='k', labelsize='30', width=3)\n",
    "\n",
    "plt.legend(fontsize='30')\n",
    "plt.show()\n",
    "\n",
    "#plt.savefig(model_name + '-1-accuracy_vs_iterations.png', dpi=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can infer from the above graph, the test accuracy reaches it maximum value at around 40,000 iterations. There is a slight decrease in it thereafter. The train accuracy however increases indefinitely. This is a clear indication that the model is overfitting the data, as the train accuracy is not reproducible in the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOSS ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''PLOTTTING THE LOSS GRAPH'''\n",
    "\n",
    "plt.figure(figsize=[39,24], dpi=50)\n",
    "\n",
    "plt.plot(iter_list, train_loss_list, '-', lw=3, c='salmon', label='Train Loss')\n",
    "plt.plot(iter_list, train_roll_loss_list, '-|r', lw=6, label='Train Loss [Roll]')\n",
    "\n",
    "plt.plot(iter_list, test_loss_list, '-', lw=3, c='brown', label='Test Loss')\n",
    "plt.plot(iter_list, test_roll_loss_list, '-|k', lw=6, label='Test Loss [Roll]')\n",
    "\n",
    "\n",
    "plt.title('LOSS vs ITERATIONS', size='60')\n",
    "plt.xlabel('Number of Iterations', size='40')\n",
    "plt.ylabel('Loss', size='40')\n",
    "\n",
    "plt.grid(True, linestyle='-.')\n",
    "plt.tick_params(labelcolor='k', labelsize='30', width=3)\n",
    "\n",
    "plt.legend(fontsize='30')\n",
    "plt.show()\n",
    "\n",
    "#plt.savefig(model_name + '-1-loss_vs_iterations.png', dpi=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results from the loss graph reinforce the accuracy graph results. For the first 10,000 itertions, both the train and test loss rapidly decrease. This indicates that the model is fitting the data well. After a sizeable number of iterations ( >40,000) the test loss has stagnated while the training loss continues to decrease. This is another indication of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''ANALYZING ACCURACY AND LOSS'''\n",
    "\n",
    "### ACCURACY ANALYSIS\n",
    "print('Accuracy Analysis : ')\n",
    "print('====================')\n",
    "\n",
    "# Getting maximum accuracy\n",
    "test_max_acc = np.max(test_acc_list)\n",
    "test_roll_max_acc = np.max(test_roll_acc_list)\n",
    "\n",
    "print(\"[Average]\")\n",
    "print('--- Maximum accuracy on test-set  : {:.2f}'.format(test_roll_max_acc))\n",
    "print('--- Iteration at maximum accuracy : {}'.format(iter_list[test_roll_acc_list.index(test_roll_max_acc)]))\n",
    "print(\"[Absolute]\")\n",
    "print('--- Maximum accuracy on test-set  : {:.2f}'.format(test_max_acc))\n",
    "print('--- Iteration at maximum accuracy : {}'.format(iter_list[test_acc_list.index(test_max_acc)]))\n",
    "\n",
    "### LOSS ANALYSIS\n",
    "print('\\nLoss Analysis : ')\n",
    "print('====================')\n",
    "\n",
    "# Getting minimum loss\n",
    "test_min_loss = np.min(test_loss_list)\n",
    "test_roll_min_loss = np.min(test_roll_loss_list)\n",
    "\n",
    "print(\"[Average]\")\n",
    "print('--- Minimum loss on test-set  : {:.2f}'.format(test_roll_min_loss))\n",
    "print('--- Iteration at minimum loss : {}'.format(iter_list[test_roll_loss_list.index(test_roll_min_loss)]))\n",
    "print(\"[Absolute]\")\n",
    "print('--- Minimum  loss on test-set  : {:.2f}'.format(test_min_loss))\n",
    "print('--- Iteration at minimum loss : {}'.format(iter_list[test_loss_list.index(test_min_loss)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STORING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''PICKLING RESULTS'''\n",
    "\n",
    "iteration_to_save = '9K'\n",
    "backup_folder = os.path.join('backup_files', model_name)\n",
    "os.makedirs(backup_folder, exist_ok=True)\n",
    "\n",
    "### EVALUATION PARAMETERS ###\n",
    "# Useful in plotting and analysing graphs \n",
    "mdict = {}\n",
    "list_name = ['iter_list', 'train_loss_list', 'train_acc_list', 'test_loss_list', 'test_acc_list']\n",
    "for i in range(len(list_name)):\n",
    "    mdict[list_name[i]] = eval(list_name[i])  \n",
    "\n",
    "fileObject = open(os.path.join(backup_folder,'{}_{}_evalP'.format(model_name, iteration_to_save)),'wb')\n",
    "pickle.dump(mdict,fileObject)   \n",
    "fileObject.close()\n",
    "\n",
    "### MODEL PARAMETERS ###\n",
    "# Useful in further training the model\n",
    "msd = model.state_dict()\n",
    "fileObject = open(os.path.join(backup_folder,'{}_{}_modelP'.format(model_name, iteration_to_save)),'wb')\n",
    "pickle.dump(msd,fileObject)   \n",
    "fileObject.close()\n",
    "\n",
    "### STATE-TIME DATA ###\n",
    "timeState = [time_begin, time_end, str(iterr)]\n",
    "fileObject = open(os.path.join(backup_folder,'{}_{}_timeState'.format(model_name, iteration_to_save)),'wb')\n",
    "pickle.dump(timeState, fileObject)   \n",
    "fileObject.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''RESTORING PICKLED RESULTS'''\n",
    "\n",
    "iteration_to_load = '90K'\n",
    "backup_folder = os.path.join('backup_files', model_name)\n",
    "os.makedirs(backup_folder, exist_ok=True)\n",
    "\n",
    "### EVALUATION PARAMETERS ###\n",
    "fileObject = open(os.path.join(backup_folder,'{}_{}_evalP'.format(model_name, iteration_to_load)),'rb')\n",
    "mdictx = pickle.load(fileObject)  \n",
    "\n",
    "iter_list = mdictx['iter_list']\n",
    "train_loss_list = mdictx['train_loss_list']\n",
    "train_acc_list = mdictx['train_acc_list']\n",
    "test_loss_list = mdictx['test_loss_list']\n",
    "test_acc_list = mdictx['test_acc_list']\n",
    "\n",
    "### MODEL PARAMETERS ###\n",
    "fileObject = open(os.path.join(backup_folder,'{}_{}_modelP'.format(model_name, iteration_to_load)),'rb')\n",
    "msd = pickle.load(fileObject) \n",
    "model.load_state_dict(state_dict = msd)\n",
    "\n",
    "### STATE-TIME DATA ###\n",
    "fileObject = open(os.path.join(backup_folder,'{}_{}_timeState'.format(model_name, iteration_to_load)),'rb')\n",
    "timeState = pickle.load(fileObject) \n",
    "\n",
    "time_begin = timeState[0]\n",
    "time_end = timeState[1]\n",
    "iterr = int(timeState[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''TRIMMING THE DATA'''\n",
    "# Useful if checkpoint measurements have been taken for values other than 'inspect_size' = 250.\n",
    "# 'factor' effectively transforms the data such that 'inspect_size' decreses by a factor of its value.\n",
    "factor = 5\n",
    "\n",
    "### Backing up the Data\n",
    "# iter_list_store = iter_list\n",
    "# train_loss_list_store = train_loss_list\n",
    "# train_acc_list_store = train_acc_list\n",
    "# test_loss_list_store = test_loss_list\n",
    "# test_acc_list_store = test_acc_list\n",
    "\n",
    "### Trimming the data\n",
    "# iter_list = [iter_list[i+factor-1] for i in range(len(iter_list)) if i%factor==0]\n",
    "# train_loss_list = [train_loss_list[i+factor-1] for i in range(len(train_loss_list)) if i%factor==0]\n",
    "# train_acc_list = [train_acc_list[i+factor-1] for i in range(len(train_acc_list)) if i%factor==0]\n",
    "# test_loss_list = [test_loss_list[i+factor-1] for i in range(len(test_loss_list)) if i%factor==0]\n",
    "# test_acc_list = [test_acc_list[i+factor-1] for i in range(len(test_acc_list)) if i%factor==0]\n",
    "\n",
    "### Verifying if the trim was succesful \n",
    "# print(len(iter_list_store), len(iter_list))\n",
    "# print(len(train_loss_list_store), len(train_loss_list))\n",
    "# print(len(train_acc_list_store), len(train_acc_list))\n",
    "# print(len(test_loss_list_store), len(test_loss_list))\n",
    "# print(len(test_acc_list_store), len(test_acc_list))\n",
    "\n",
    "# print(iter_list[:7],'\\t', iter_list[-7:])\n",
    "# print(iter_list_store[:7],'\\t', iter_list_store[-7:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
